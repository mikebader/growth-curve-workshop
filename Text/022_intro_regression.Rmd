---
title: "Estimates with Regression"
author: "Mike Bader"
date: "June 24, 2016"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
    toc_depth: 3
bibliography: /Users/bader/work/Bibs/bib20100831.bib
---
```{r setup,include=FALSE}
if(file.exists('../_init.R')) (source('../_init.R'))
zillow.apr16 <- read.csv("Data/zillowApr16.csv",header=TRUE)
```

## Moving from Mean to Regression

Calculating the univariate mean, as we just did, does not help us much. Our main interest in epidemiology is to find the association between one variable and another. In order to do that, we typically use regression. 

## Generate Fake Data of Home Values by Metro Size

We will start by using the same variable that we discussed previously, the median price per square foot of housing in metropolitan areas. I suspect that more populous metropolitan areas have higher home prices than less populous metros. This provides a simple model, one with which we are all familiar: 

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

This represents the linear regression model where $y_i$ represents our outcome variable, price per square foot. Notice the subscript $i$ there that tells us that each unit (metropolitan area in our case) takes on its own value. We estimate the outcome as a function of the intercept, $\beta_0$, and the slope of a line, $\beta_1$. Notice that neither of these variables includes a subscript, which means that we are indicating that they are the same for all units. The slope of the line predicts the value of $y_i$ given the unit's value of $x_i$. But, just as the mean was our best guess, now the combination of $\beta_0 + \beta_1$ gives us a more sophisticated best guess -- but the actual value between observed $y_i$ and $x_i$ for each unit $i$ will differ. The final term, $\epsilon_i$ represents this difference. 

We call the $\beta_0$ and $\beta_1$ the **intercept** and the **slope**, respectively. The intercept represents the value when $x_i$ equals zero and the line crosses, or intercepts, the $y$-axis. The slope represents the value of the rise over the run and represents the increase in price for each unit change in the $x$ variable. This is likely review, but these terms will become important in a little bit, so I just want to make sure that they are clear. 


### Write Model of Fake Relationship Between Home Price and Population Size

Now we can move on to specifying a model based on a hypothesized set of parameters. In the case of median price and population size, I anticipate that the percentage change in population size will predict a percentage change in median home value per square foot (this is what economists term [elasticity][]). To model this kind of change, we take the logarithm of both variables (an explanation for why can be found at the bottom of [this page][ats-elasticity]). I will create a population described by the process: 

$$\ln(y_i) = \beta_0 + \beta_1 \ln(x_i) + \epsilon_i$$

One problem with this model is that $\beta_0$ represents the value when the metro population equals zero. That doesn't make a whole lot of sense, so we might want to "center" the population around some meaningful value. This will often be the mean, which makes $\beta_0$ the "conditional mean" of price; that is, the price at the mean population size net of the influence of population on price. Let's do something different and write the equation to reflect the difference from a substantively interesting value like, say, the population of the New York metro area (which is about 20.2 million people). I will make a population where the median price per square foot of the metro area increases by half of a percent for every one percent increase in the population of the metro area and the median value equals $180 at the intercept (the price per square foot of real estate in the New York metro, the natural logarithm of which equals 5.19):

$$\ln(price_i) = 5.19 + 0.5\times\left[\ln(pop_i)-\ln(20.2\times10^6)\right] + \epsilon_i$$ 

[elasticity]: https://en.wikipedia.org/wiki/Elasticity_(economics) "Definition of Elasticity"
[ats-elasticity]: http://www.ats.ucla.edu/stat/sas/faq/sas_interpret_log.htm "Interpretation of elasticity from regression models"

### Create Our Fake Population of Metropolitan Areas with Price and Population Size

Now we can input these variables into R to create our population:

```{r size-price-cross-section, echo=FALSE}
library(dplyr)
library(RCurl)
set.seed(5831209)                       # This ensures that we get the same results every time
N                   <- 150              # 150 Metro Areas
beta_0              <- log(180)         # Log of $180
beta_1              <- 0.5
pop_i               <- sort(rlnorm(N, meanlog=13.2323, sdlog=1.0957))
pop_i[150]          <- 20.2e6           # Replace largest metro to equal NY metro population
e_i                 <- rnorm(N,0,0.1)   # Makes the residual with a mean zero, sd of 0.1

lnprice_i           <- beta_0 + beta_1*(log(pop_i)-log(20.2e6)) + e_i
price_i             <- exp(lnprice_i)
price.size.data     <- data.frame(i=c(1:N),beta_0,beta_1,pop_i,e_i,price_i,lnprice_i)
```

And let's look at our data: 

```{r size-price-data}
price.size.data[c(1:5,146:150),]
plot(log(price.size.data$pop_i),price.size.data$lnprice_i)
```

### Analyze Fake Relationship Between Home Price and Population Size

Look again at the data printed above. Remember, we know the real values of $\beta_0$ and $\beta_1$ because we played god and make them that way. If we just collected these data (from a population, we are not dealing with sampling), all we would be able to observe would be: 

```{r}
price.size.data[c(1:5,146:150),c('i','price_i','pop_i')]
```

In order to estimate what process generated these data, we analyze the data based on a regression model that estimates the value of the parameter based on observed data. We would then specify the parameters we want to estimate and then go about estimating them. Recall that this is the same thing that we did with the mean, its just that we now have a slightly more complicated estimate. We would specify our model: 

$$\ln(price_i) = \beta_0 + \beta_1\times\left[\ln(pop_i)-\ln(20.2\times10^6)\right] + \epsilon_i$$ 

then estimate that model and show the line of best fit on the scatter plot to check our work: 

```{r price-size-lm}
price.size.model <- lm(lnprice_i ~ I(log(pop_i) - log(20.2e6)))
summary(price.size.model)
```

```{r coef, include=FALSE}
coef <- round(price.size.model$coefficients,2)
```

*Et voila!* We get estimates very similar to the actual population parameters that we specified in the model. Let's interpret what this means. The intercept, our estimate of $\beta_0$, equals `r coef[1]`. Since we centered this at the population of the largest city (x=0 when the population equals 20.2 million), this means that we estimated the median home value to be $e^{`r coef[1]`}=`r round(exp(coef[1]),2)`$ USD in the largest city (our fake New York). The slope, our estimate of $\beta_1$, equals `r coef[2]`. This represents the elasticity of price by metro size: for every one percent increase in the size of the metro population, we expect the median home value per square foot to increase `r coef[2]` percent. 

Now let's plot the data and the regression line that we just fit: 

``` {r price-size-fitplot-bad}
plot(log(price.size.data$pop_i),price.size.data$lnprice_i)
abline(price.size.model$coefficients)
```

Uh oh! There is no line! Did we make a mistake? No: remember that we set the intercept in our model to equal the (logged) population of the New York metropolitan area. We need to shift our x-intercept to reflect the model that we ran:

``` {r price-size-fitplot-good}
plot(log(price.size.data$pop_i)-log(20.2e6),price.size.data$lnprice_i)
abline(price.size.model$coefficients)
```

Now we can find the error that exists from the model. Let's add columns representing our estimates of $\beta_0$ & $\beta_1$ to our dataset (represented by the columns `b0` and `b1`) and then calculate our error from the model (`ehat`, which we get by solving for $e_i$ in the regression equation) :

```{r price-size-error-data}
price.size.data <- mutate(price.size.data,
                           b0 = price.size.model$coefficients[1]
                          ,b1 = price.size.model$coefficients[2]
                          ,ehat = lnprice_i - (b0 + b1*(log(pop_i)-log(20.2e6)))
                          )
round(price.size.data[c(1:5,146:150),],3)
```

If you look at the value of `ehat` in the data, it equals the distance from the observed point (circle) on the plot to the line. A positive value means that the model underestimated the (logged) price and a negative value means that the model overestimated the (logged) price. We can then look at the errors:

```{r price-size-errors}
hist(price.size.data$ehat,breaks=10)
price.size.sd <- sd(price.size.data$ehat)
price.size.sd
pct.1sd <- sum(
    price.size.data$ehat >= -price.size.sd 
    & price.size.data$ehat <= price.size.sd)/N
pct.1sd
```
The standard deviation of the errors approximately equals the parameter, and the errors are approximately normally distributed with `r 100*round(pct.1sd,2)`% within $\pm$ 1s.d. The standard deviation means that we expect that `r 100*round(pct.1sd,2)`% of metropolitan-level estimates of prices will fall within `r 100*round(price.size.sd,3)`% of the price expected from the model.

## Introducing Real Home Price and Metro Population Data

Now we turn from modeling our fake data that we generated in our own little sandbox of a world, to analyzing real-world data. We will use the same data that we used in the previous section. To analyze the data, however, we also need to append metro population data to the dataset.

### Gather Data from Zillow and American Community Survey
The first issue is that we need to connect the Zillow region IDs to the MSA codes used in Census data. Fortunately [Zillow publishes this crosswalk][crosswalk]. We first read this data and merge it to our data from April 2016 from Zillow. Next, we get ACS data from a file that I downloaded from [Social Explorer][se] and available on my website. Then, we merge those two files together. 

[crosswalk]: http://files.zillowstatic.com/research/public/CountyCrossWalk_Zillow.csv "Zillow to FIPS crosswalk file"
[se]: http://socialexplorer.com "Social Explorer"

```{r merge-acs}
xwlk.url <- 'http://files.zillowstatic.com/research/public/CountyCrossWalk_Zillow.csv'
xwlk <- read.csv(xwlk.url,header=TRUE)[,c('CBSAName','MetroRegionID_Zillow', 'CBSACode')]
xwlk <- xwlk[!duplicated(xwlk),]

zillow.acs <- merge(zillow.apr16[,c('RegionID','RegionName','X2016.04')],xwlk,
                    by.x='RegionID',by.y='MetroRegionID_Zillow')

f.url <- 'https://raw.githubusercontent.com/mikebader/teaching-growth-curve-workshop/master/Data/R11198946_SL310.csv'
acs <- read.csv(f.url,header=T)[,c("Geo_FIPS","Geo_NAME","Geo_DIVISION","SE_T001_001","SE_T057_001")]

zillow.acs <- merge(zillow.acs,acs,by.x='CBSACode',by.y='Geo_FIPS')
zillow.acs <- mutate(zillow.acs,
                 price_i = X2016.04
                ,pop_i = SE_T001_001
                ,lnprice_i = log(X2016.04)
                ,lnpop_i = log(pop_i)
                ,i = 1:150
                )
zillow.acs[c(1:5,146:150),c('i','price_i','pop_i','lnprice_i','lnpop_i')]
```

### Analyze Zillow and ACS Data

If you look back a few sections, you will see that these data mimic those that we created. Now we are in a position to analyze the data. Recall that both median home prices per square feet and population size tend to be exponentially distributed and we would like to estimate the elasticity of the model. Hence, I took the log of both price and population. 

Let's look at a plot of the data: 

```{r zillow-acs-plots}
plot(zillow.acs$lnpop_i,zillow.acs$lnprice_i)
```

A lot more dispersed than our old data, but still trending positive. Now we analyze the combined data with the model of elasticities centering the model so that the intercept equals the estimated home value in New York City: 

```{r zillow-acs-cross-section}
real.price.model <- lm(lnprice_i ~ I(lnpop_i - log(20.2e6)),data=zillow.acs)
summary(real.price.model)
real.coef <- real.price.model$coefficients
```

### Interpret Zillow and ACS data

This model tells us that the median price per square foot of homes in the New York metro in April 2016 was $e^{`r round(real.coef[1],2)`} =$ $\$`r round(exp(real.coef[1]),0)`$. We would expect a metro area with a one percent larger population than another to have home values `r round(real.coef[2],2)`% higher. Let's re-plot the data with this estimation line included: 

```{r zillow-acs-cross-prediction-plot}
plot(zillow.acs$lnpop_i-log(20.2e6),zillow.acs$lnprice_i)
abline(real.coef)
```

Now we can also calculate the errors off of the trend line, the variation unique to each metropolitan area: 

```{r zillow-acs-errors}
zillow.acs <- mutate(zillow.acs
                , b0 = real.coef[1]
                , b1 = real.coef[2]
                , ehat = lnprice_i - (b0 + b1*(lnpop_i-log(20.2e6)))
)
zillow.acs[c(1:5,146:150),c('i','lnprice_i','lnpop_i','b0','b1','ehat')]
hist(zillow.acs$ehat,breaks=12)
real.price.sd <- sd(zillow.acs$ehat)
pct.1sd <- sum(
    zillow.acs$ehat >= -real.price.sd 
    & zillow.acs$ehat <= real.price.sd)/N
pct.1sd
```

We can see that, once again, our model does not fit the data as well as it could. The errors are not centered on zero as they should be and they are skewed right. This could mean that we are missing important variables in the model or could indicate that we need to find a better transformation to make the relationship between price and size more linear. 


